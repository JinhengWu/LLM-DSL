{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "d:\\Anaconda3\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e4b739d34d4eb1b14a530e1718f61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.9827, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 12.7633, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 12.607, 'learning_rate': 3e-06, 'epoch': 0.12}\n",
      "{'loss': 12.4995, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 12.1786, 'learning_rate': 5e-06, 'epoch': 0.2}\n",
      "{'loss': 11.6315, 'learning_rate': 6e-06, 'epoch': 0.24}\n",
      "{'loss': 10.9335, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 10.2722, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 9.4398, 'learning_rate': 9e-06, 'epoch': 0.36}\n",
      "{'loss': 8.6201, 'learning_rate': 1e-05, 'epoch': 0.4}\n",
      "{'loss': 7.3362, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 6.4139, 'learning_rate': 1.2e-05, 'epoch': 0.48}\n",
      "{'loss': 5.4191, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.52}\n",
      "{'loss': 4.0274, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 3.4115, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n",
      "{'loss': 2.8153, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 2.3622, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 1.8868, 'learning_rate': 1.8e-05, 'epoch': 0.72}\n",
      "{'loss': 1.8716, 'learning_rate': 1.9e-05, 'epoch': 0.76}\n",
      "{'loss': 1.4389, 'learning_rate': 2e-05, 'epoch': 0.8}\n",
      "{'loss': 1.2777, 'learning_rate': 2.1e-05, 'epoch': 0.84}\n",
      "{'loss': 1.0975, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8861, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 0.7195, 'learning_rate': 2.4e-05, 'epoch': 0.96}\n",
      "{'loss': 0.6324, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "{'loss': 0.5507, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4571, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3958, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3303, 'learning_rate': 2.9e-05, 'epoch': 1.16}\n",
      "{'loss': 0.298, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.258, 'learning_rate': 3.1e-05, 'epoch': 1.24}\n",
      "{'loss': 0.2169, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1889, 'learning_rate': 3.3e-05, 'epoch': 1.32}\n",
      "{'loss': 0.1602, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.36}\n",
      "{'loss': 0.1457, 'learning_rate': 3.5e-05, 'epoch': 1.4}\n",
      "{'loss': 0.123, 'learning_rate': 3.6e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1137, 'learning_rate': 3.7e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0953, 'learning_rate': 3.8e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0804, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0731, 'learning_rate': 4e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0673, 'learning_rate': 4.1e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0597, 'learning_rate': 4.2e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0557, 'learning_rate': 4.3e-05, 'epoch': 1.72}\n",
      "{'loss': 0.047, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0436, 'learning_rate': 4.5e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0383, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0333, 'learning_rate': 4.7e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0322, 'learning_rate': 4.8e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0276, 'learning_rate': 4.9e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0264, 'learning_rate': 5e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0255, 'learning_rate': 4.8e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0249, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0233, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0239, 'learning_rate': 4.2e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0196, 'learning_rate': 4e-05, 'epoch': 2.2}\n",
      "{'loss': 0.021, 'learning_rate': 3.8e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0174, 'learning_rate': 3.6e-05, 'epoch': 2.28}\n",
      "{'loss': 0.019, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0158, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0174, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0196, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0155, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0152, 'learning_rate': 2.4e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0158, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0159, 'learning_rate': 2e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0149, 'learning_rate': 1.8e-05, 'epoch': 2.64}\n",
      "{'loss': 0.0144, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0134, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.72}\n",
      "{'loss': 0.014, 'learning_rate': 1.2e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0135, 'learning_rate': 1e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0138, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0152, 'learning_rate': 6e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0139, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0131, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.96}\n",
      "{'loss': 0.0147, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 540.8703, 'train_samples_per_second': 22.186, 'train_steps_per_second': 1.387, 'train_loss': 2.1316490769584973, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=2.1316490769584973, metrics={'train_runtime': 540.8703, 'train_samples_per_second': 22.186, 'train_steps_per_second': 1.387, 'train_loss': 2.1316490769584973, 'epoch': 3.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = 'Train.csv'\n",
    "data = pd.read_csv(train_file_path)\n",
    "\n",
    "data = data.sample(n=5000, random_state=42)\n",
    "\n",
    "train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.1111, random_state=42)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class NLtoDSLDataSet(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = item['input']\n",
    "        target_text = item['output']\n",
    "        \n",
    "        input_encoding = self.tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        target_encoding = self.tokenizer(target_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        \n",
    "        inputs = {\n",
    "            'input_ids': input_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
    "            'labels': target_encoding['input_ids'].flatten()\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "train_dataset = NLtoDSLDataSet(tokenizer, train_data)\n",
    "val_dataset = NLtoDSLDataSet(tokenizer, val_data)\n",
    "\n",
    "# Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    do_train=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\"\n",
    ") \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL Strategy Results: {'exact_match_rate': 0.02}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def select_top_k_samples(input_text, train_data, k=500, max_length=128):\n",
    "    input_vec = tokenizer(input_text, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].to(device)\n",
    "    similarities = []\n",
    "    for idx, row in train_data.iterrows():\n",
    "        train_vec = tokenizer(row['input'], max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')['input_ids'].to(device)\n",
    "        sim = cosine_similarity(input_vec.cpu().numpy(), train_vec.cpu().numpy())\n",
    "        similarities.append((sim, row['output']))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_k_samples = [sample for _, sample in similarities[:k]]\n",
    "    return top_k_samples\n",
    "\n",
    "def icl_inference(test_data, max_length=128, max_new_tokens=50):\n",
    "    results = {\"exact_match_rate\": 0}\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        input_text = row['input']\n",
    "        selected_samples = select_top_k_samples(input_text, train_data, max_length=max_length)\n",
    "        \n",
    "        context = \" \".join(selected_samples)\n",
    "        inputs = tokenizer(context + \" \" + input_text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        outputs = model.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "        generated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if generated_output == row['output']:\n",
    "            results[\"exact_match_rate\"] += 1\n",
    "    \n",
    "    results[\"exact_match_rate\"] /= len(test_data)\n",
    "    return results\n",
    "\n",
    "\n",
    "icl_results = icl_inference(test_data)\n",
    "print(\"ICL Strategy Results:\", icl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Inference Results: {'exact_match_rate': 0.00}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set using standard inference\n",
    "def standard_inference(test_data):\n",
    "    results = {\"exact_match_rate\": 0}\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        input_text = row['input']\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        outputs = model.generate(inputs)\n",
    "        generated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Compare generated output with expected output\n",
    "        if generated_output == row['output']:\n",
    "            results[\"exact_match_rate\"] += 1\n",
    "    \n",
    "    results[\"exact_match_rate\"] /= len(test_data)\n",
    "    return results\n",
    "\n",
    "# Evaluate\n",
    "standard_results = standard_inference(test_data)\n",
    "print(\"Standard Inference Results:\", standard_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Samples using MMR:\n",
      "Allocate 50% to large-cap stocks.\n",
      "Assign 30% to small-cap stocks.\n",
      "Increase large-cap stocks by 10%.\n",
      "Selected Samples using DPP:\n",
      "Set 20% in bonds.\n",
      "Assign 30% to small-cap stocks.\n",
      "Increase large-cap stocks by 10%.\n",
      "Similarity Matrix:\n",
      "[[1.         0.52530374 0.33548247 0.09345357 0.04205372]\n",
      " [0.52530374 1.         0.20097644 0.10252896 0.26431731]\n",
      " [0.33548247 0.20097644 1.         0.31283241 0.44881165]\n",
      " [0.09345357 0.10252896 0.31283241 1.         0.64050891]\n",
      " [0.04205372 0.26431731 0.44881165 0.64050891 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "sample_data = [\n",
    "    {\"input\": \"Allocate 50% to large-cap stocks.\"},\n",
    "    {\"input\": \"Assign 30% to small-cap stocks.\"},\n",
    "    {\"input\": \"Set 20% in bonds.\"},\n",
    "    {\"input\": \"Increase large-cap stocks by 10%.\"},\n",
    "    {\"input\": \"Move 15% to real estate investments.\"}\n",
    "]\n",
    "\n",
    "def calculate_similarity(input_text, sample_text, tokenizer):\n",
    "    input_vec = tokenizer(input_text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)[\"input_ids\"]\n",
    "    sample_vec = tokenizer(sample_text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)[\"input_ids\"]\n",
    "    return cosine_similarity(input_vec, sample_vec)[0][0]\n",
    "\n",
    "def mmr(input_text, candidate_samples, tokenizer, lambda_param=0.5, k=3):\n",
    "    selected_samples = []\n",
    "    candidate_scores = {}\n",
    "\n",
    "\n",
    "    for sample in candidate_samples:\n",
    "        sim_score = calculate_similarity(input_text, sample[\"input\"], tokenizer)\n",
    "        candidate_scores[sample[\"input\"]] = sim_score\n",
    "\n",
    "    while len(selected_samples) < k and candidate_scores:\n",
    "\n",
    "        best_candidate = max(candidate_scores, key=candidate_scores.get)\n",
    "        selected_samples.append(best_candidate)\n",
    "        del candidate_scores[best_candidate]\n",
    "\n",
    "        for candidate in candidate_scores:\n",
    "            candidate_scores[candidate] = lambda_param * calculate_similarity(input_text, candidate, tokenizer) - \\\n",
    "                                          (1 - lambda_param) * max(calculate_similarity(candidate, s, tokenizer) for s in selected_samples)\n",
    "\n",
    "    return selected_samples\n",
    "\n",
    "\n",
    "def dpp(candidate_samples, tokenizer, k=3):\n",
    "    selected_samples = []\n",
    "    similarity_matrix = np.zeros((len(candidate_samples), len(candidate_samples)))\n",
    "\n",
    "    for i, sample_i in enumerate(candidate_samples):\n",
    "        for j, sample_j in enumerate(candidate_samples):\n",
    "            similarity_matrix[i, j] = calculate_similarity(sample_i[\"input\"], sample_j[\"input\"], tokenizer)\n",
    "\n",
    "    while len(selected_samples) < k:\n",
    "        if not selected_samples:\n",
    "            selected_samples.append(candidate_samples[np.argmax(np.diag(similarity_matrix))][\"input\"])\n",
    "        else:\n",
    "            scores = []\n",
    "            for i, sample in enumerate(candidate_samples):\n",
    "                if sample[\"input\"] not in selected_samples:\n",
    "                    selected = selected_samples + [sample[\"input\"]]\n",
    "                    indices = [candidate_samples.index({\"input\": s}) for s in selected]\n",
    "                    sub_matrix = similarity_matrix[np.ix_(indices, indices)]\n",
    "                    scores.append((sample[\"input\"], np.linalg.det(sub_matrix)))\n",
    "\n",
    "            best_candidate = max(scores, key=lambda x: x[1])[0]\n",
    "            selected_samples.append(best_candidate)\n",
    "\n",
    "    return selected_samples\n",
    "\n",
    "input_text = \"Allocate more to large-cap stocks.\"\n",
    "selected_samples_mmr = mmr(input_text, sample_data, tokenizer, lambda_param=0.5, k=3)\n",
    "\n",
    "print(\"Selected Samples using MMR:\")\n",
    "for sample in selected_samples_mmr:\n",
    "    print(sample)\n",
    "\n",
    "\n",
    "selected_samples_dpp = dpp(sample_data, tokenizer, k=3)\n",
    "\n",
    "print(\"Selected Samples using DPP:\")\n",
    "for sample in selected_samples_dpp:\n",
    "    print(sample)\n",
    "\n",
    "print_similarity_matrix(sample_data, tokenizer)\n",
    "\n",
    "assert selected_samples_mmr[0] == \"Allocate 50% to large-cap stocks.\", \"MMR Test Failed!\"\n",
    "assert selected_samples_dpp[0] != selected_samples_dpp[1], \"DPP Test Failed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\n",
      "Random Samples: ['Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds'\n",
      " 'Please set small-size stocks by 84 proportion in ValueInvest.'\n",
      " 'Could you alter 23 proportion to fixed-income securities in HighYieldBonds?']\n",
      "MMR Samples: ['Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds', 'Please alter minor stocks by 6 percent in myPortfolio.', 'assigning GreenEnergyInvest to include 66% more big-cap stocks.']\n",
      "DPP Samples: ['Please alter minor stocks by 6 percent in myPortfolio.', \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", 'Could you set 74% to bonds in CryptoAssets?']\n",
      "----------------------------------------\n",
      "Input: Please alter minor stocks by 6 percent in myPortfolio.\n",
      "Random Samples: [\"I'm considering adjusting VentureCapital with an additional 66 percentage of minor stocks.\"\n",
      " \"Let's alter 62 percent more of mid-cap stocks to RealEstateHoldings's portfolio.\"\n",
      " 'assign PreciousMetalsFund by adding 4 percent small-size stocks, please.']\n",
      "MMR Samples: ['Please alter minor stocks by 6 percent in myPortfolio.', 'Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds', \"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\"]\n",
      "DPP Samples: ['Please alter minor stocks by 6 percent in myPortfolio.', \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", 'Could you set 74% to bonds in CryptoAssets?']\n",
      "----------------------------------------\n",
      "Input: We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\n",
      "Random Samples: ['Please assign bonds by 57 percent in VentureCapital.'\n",
      " 'Please change small-size stocks by 57% in RealEstateHoldings.'\n",
      " 'Thinking of altering 14% debt instruments into TechGrowth']\n",
      "MMR Samples: [\"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\", 'Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds', \"Let's assign 58 percentage more of debt instruments to TechGrowth's portfolio.\"]\n",
      "DPP Samples: ['Please alter minor stocks by 6 percent in myPortfolio.', \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", 'Could you set 74% to bonds in CryptoAssets?']\n",
      "----------------------------------------\n",
      "Input: assigning GreenEnergyInvest to include 66% more big-cap stocks.\n",
      "Random Samples: ['set 63% to minor stocks in ValueInvest'\n",
      " \"Let's assign 43 percentage more of bonds to IncomeFund2024's portfolio.\"\n",
      " 'Thinking of changeing 20 percent major stocks into GlobalEquityFund']\n",
      "MMR Samples: ['assigning GreenEnergyInvest to include 66% more big-cap stocks.', 'Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds', \"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\"]\n",
      "DPP Samples: ['Please alter minor stocks by 6 percent in myPortfolio.', \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", 'Could you set 74% to bonds in CryptoAssets?']\n",
      "----------------------------------------\n",
      "Input: Could you alter 23 proportion to fixed-income securities in HighYieldBonds?\n",
      "Random Samples: ['allocate PreciousMetalsFund by adding 14% fixed-income securities, please.'\n",
      " \"Is it possible to amend VentureCapital's large-cap stocks allocation by 53 proportion?\"\n",
      " 'designate IncomeFund2024 by adding 76 proportion mid-size stocks, please.']\n",
      "MMR Samples: ['Could you alter 23 proportion to fixed-income securities in HighYieldBonds?', 'Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds', 'Please alter minor stocks by 6 percent in myPortfolio.']\n",
      "DPP Samples: ['Please alter minor stocks by 6 percent in myPortfolio.', \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", 'Could you set 74% to bonds in CryptoAssets?']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('Train.csv')\n",
    "\n",
    "# 仅使用前100条数据进行测试\n",
    "test_data = data.head(100)\n",
    "\n",
    "# 计算相似度函数\n",
    "def calculate_similarity(input_text, sample_text, tokenizer):\n",
    "    input_vec = tokenizer(input_text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)[\"input_ids\"]\n",
    "    sample_vec = tokenizer(sample_text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)[\"input_ids\"]\n",
    "    return cosine_similarity(input_vec, sample_vec)[0][0]\n",
    "\n",
    "# MMR算法\n",
    "def mmr(input_text, candidate_samples, tokenizer, lambda_param=0.5, k=3):\n",
    "    selected_samples = []\n",
    "    candidate_scores = {}\n",
    "\n",
    "    for sample in candidate_samples:\n",
    "        sim_score = calculate_similarity(input_text, sample, tokenizer)\n",
    "        candidate_scores[sample] = sim_score\n",
    "\n",
    "    while len(selected_samples) < k and candidate_scores:\n",
    "        best_candidate = max(candidate_scores, key=candidate_scores.get)\n",
    "        selected_samples.append(best_candidate)\n",
    "        del candidate_scores[best_candidate]\n",
    "\n",
    "        for candidate in candidate_scores:\n",
    "            candidate_scores[candidate] = lambda_param * calculate_similarity(input_text, candidate, tokenizer) - \\\n",
    "                                          (1 - lambda_param) * max(calculate_similarity(candidate, s, tokenizer) for s in selected_samples)\n",
    "\n",
    "    return selected_samples\n",
    "\n",
    "# DPP算法\n",
    "def dpp(candidate_samples, tokenizer, k=3):\n",
    "    selected_samples = []\n",
    "    similarity_matrix = np.zeros((len(candidate_samples), len(candidate_samples)))\n",
    "\n",
    "    for i, sample_i in enumerate(candidate_samples):\n",
    "        for j, sample_j in enumerate(candidate_samples):\n",
    "            similarity_matrix[i, j] = calculate_similarity(sample_i, sample_j, tokenizer)\n",
    "\n",
    "    while len(selected_samples) < k:\n",
    "        if not selected_samples:\n",
    "            selected_samples.append(candidate_samples[np.argmax(np.diag(similarity_matrix))])\n",
    "        else:\n",
    "            scores = []\n",
    "            for i, sample in enumerate(candidate_samples):\n",
    "                if sample not in selected_samples:\n",
    "                    selected = selected_samples + [sample]\n",
    "                    indices = [candidate_samples.index(s) for s in selected]\n",
    "                    sub_matrix = similarity_matrix[np.ix_(indices, indices)]\n",
    "                    scores.append((sample, np.linalg.det(sub_matrix)))\n",
    "\n",
    "            best_candidate = max(scores, key=lambda x: x[1])[0]\n",
    "            selected_samples.append(best_candidate)\n",
    "\n",
    "    return selected_samples\n",
    "\n",
    "# 消融性测试\n",
    "def ablation_test(test_data, tokenizer, mmr_lambda=0.5, k=3):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in test_data.iterrows():\n",
    "        input_text = row['input']\n",
    "        candidate_samples = test_data['input'].tolist()\n",
    "\n",
    "        # 随机选择样本 (Baseline)\n",
    "        random_samples = np.random.choice(candidate_samples, k, replace=False)\n",
    "\n",
    "        # MMR选择样本\n",
    "        mmr_samples = mmr(input_text, candidate_samples, tokenizer, lambda_param=mmr_lambda, k=k)\n",
    "\n",
    "        # DPP选择样本\n",
    "        dpp_samples = dpp(candidate_samples, tokenizer, k=k)\n",
    "\n",
    "        # 记录结果\n",
    "        results.append({\n",
    "            'input': input_text,\n",
    "            'random_samples': random_samples,\n",
    "            'mmr_samples': mmr_samples,\n",
    "            'dpp_samples': dpp_samples\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# 执行消融性测试\n",
    "ablation_results = ablation_test(test_data, tokenizer, mmr_lambda=0.5, k=3)\n",
    "\n",
    "# 打印部分测试结果\n",
    "for result in ablation_results[:5]:\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Random Samples: {result['random_samples']}\")\n",
    "    print(f\"MMR Samples: {result['mmr_samples']}\")\n",
    "    print(f\"DPP Samples: {result['dpp_samples']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Samples Average BLEU: 0.0017\n",
      "MMR Samples Average BLEU: 0.0070\n",
      "DPP Samples Average BLEU: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "    return score\n",
    "\n",
    "input_texts = [\n",
    "    \"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\",\n",
    "    \"Please alter minor stocks by 6 percent in myPortfolio.\",\n",
    "    \"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\",\n",
    "    \"assigning GreenEnergyInvest to include 66% more big-cap stocks.\",\n",
    "    \"Could you alter 23 proportion to fixed-income securities in HighYieldBonds?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"SET ETF HighYieldBonds WITH SMALL_CAP_STOCKS = 33%\",\n",
    "    \"UPDATE ETF myPortfolio WITH SMALL_CAP_STOCKS = 6%\",\n",
    "    \"UPDATE ETF BlueChipStocks WITH BONDS = 11%\",\n",
    "    \"SET ETF GreenEnergyInvest WITH LARGE_CAP_STOCKS = 66%\",\n",
    "    \"UPDATE ETF HighYieldBonds WITH BONDS = 23%\"\n",
    "]\n",
    "\n",
    "random_samples = [\n",
    "    [\"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\", \"Please set small-size stocks by 84 proportion in ValueInvest.\", \"Could you alter 23 proportion to fixed-income securities in HighYieldBonds?\"],\n",
    "    [\"I'm considering adjusting VentureCapital with an additional 66 percentage of minor stocks.\", \"Let's alter 62 percent more of mid-cap stocks to RealEstateHoldings's portfolio.\", \"assign PreciousMetalsFund by adding 4 percent small-size stocks, please.\"],\n",
    "    [\"Please assign bonds by 57 percent in VentureCapital.\", \"Please change small-size stocks by 57% in RealEstateHoldings.\", \"Thinking of altering 14% debt instruments into TechGrowth\"],\n",
    "    [\"set 63% to minor stocks in ValueInvest\", \"Let's assign 43 percentage more of bonds to IncomeFund2024's portfolio.\", \"Thinking of changeing 20 percent major stocks into GlobalEquityFund\"],\n",
    "    [\"allocate PreciousMetalsFund by adding 14% fixed-income securities, please.\", \"Is it possible to amend VentureCapital's large-cap stocks allocation by 53 proportion?\", \"designate IncomeFund2024 by adding 76 proportion mid-size stocks, please.\"]\n",
    "]\n",
    "\n",
    "mmr_samples = [\n",
    "    [\"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\", \"Please alter minor stocks by 6 percent in myPortfolio.\", \"assigning GreenEnergyInvest to include 66% more big-cap stocks.\"],\n",
    "    [\"Please alter minor stocks by 6 percent in myPortfolio.\", \"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\", \"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\"],\n",
    "    [\"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\", \"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\", \"Let's assign 58 percentage more of debt instruments to TechGrowth's portfolio.\"],\n",
    "    [\"assigning GreenEnergyInvest to include 66% more big-cap stocks.\", \"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\", \"We're moving to amend 11% more to fixed-income securities in BlueChipStocks, correct?\"],\n",
    "    [\"Could you alter 23 proportion to fixed-income securities in HighYieldBonds?\", \"Thinking of allocateing 33 percentage small-cap stocks into HighYieldBonds\", \"Please alter minor stocks by 6 percent in myPortfolio.\"]\n",
    "]\n",
    "\n",
    "dpp_samples = [\n",
    "    [\"Please alter minor stocks by 6 percent in myPortfolio.\", \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", \"Could you set 74% to bonds in CryptoAssets?\"],\n",
    "    [\"Please alter minor stocks by 6 percent in myPortfolio.\", \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", \"Could you set 74% to bonds in CryptoAssets?\"],\n",
    "    [\"Please alter minor stocks by 6 percent in myPortfolio.\", \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", \"Could you set 74% to bonds in CryptoAssets?\"],\n",
    "    [\"Please alter minor stocks by 6 percent in myPortfolio.\", \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", \"Could you set 74% to bonds in CryptoAssets?\"],\n",
    "    [\"Please alter minor stocks by 6 percent in myPortfolio.\", \"Is it possible to designate AlphaFund's fixed-income securities allocation by 23 percentage?\", \"Could you set 74% to bonds in CryptoAssets?\"]\n",
    "]\n",
    "\n",
    "def calculate_average_bleu(ground_truth, samples):\n",
    "    scores = [calculate_bleu(ground_truth, sample) for sample in samples]\n",
    "    return np.mean(scores)\n",
    "\n",
    "random_bleu_scores = [calculate_average_bleu(gt, rs) for gt, rs in zip(ground_truths, random_samples)]\n",
    "mmr_bleu_scores = [calculate_average_bleu(gt, ms) for gt, ms in zip(ground_truths, mmr_samples)]\n",
    "dpp_bleu_scores = [calculate_average_bleu(gt, ds) for gt, ds in zip(ground_truths, dpp_samples)]\n",
    "\n",
    "print(f\"Random Samples Average BLEU: {np.mean(random_bleu_scores):.4f}\")\n",
    "print(f\"MMR Samples Average BLEU: {np.mean(mmr_bleu_scores):.4f}\")\n",
    "print(f\"DPP Samples Average BLEU: {np.mean(dpp_bleu_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
